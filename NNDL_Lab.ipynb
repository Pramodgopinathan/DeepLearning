{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pramodgopinathan/DeepLearning/blob/main/NNDL_Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab1: Simple Neural Network (Perceptron)"
      ],
      "metadata": {
        "id": "XTG25TmH6Yz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Program #1: Implementation of Simple neural network with activation function\n",
        "\n",
        "![DeepLearning-Lab1Part1.png](data:image/png;base64,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)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-Xvg7r5_633w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we have two parameter one is to identify y or z which is \n",
        "\n",
        "# y = (x1 * W1 ) + (x2 * W2) + (x3 * w3) + baise\n",
        "# n = 3 (as because there are three input nerons givens)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Above example x [0.8, 0.6, 0.4], w [0.1,0.3,-0.2] and baise = 0.35\n",
        "\n",
        "X = np.array([0.8,0.6,0.4])\n",
        "W = np.array([0.1,0.3,-0.2])\n",
        "baise = 0.35\n",
        "\n",
        "print('--------------------------------------------------------------------------------------------') \n",
        "print('------------------------Finding Y output ---------------------------------------------------')\n",
        "print('--------------------------------------------------------------------------------------------')\n",
        "Y = np.dot(X,W) + baise \n",
        "print('The input of a simple neuron Y for the given network is:' , X)\n",
        "print('The weight of a simple neuron Y for the given network is:' , W)\n",
        "print('The output of a simple neuron Y for the given network is:' , Y)\n",
        "\n",
        "# Define in the problem statement \n",
        "theta = 0 \n",
        "\n",
        "def binary_step(y):\n",
        "      if y < theta:\n",
        "            return theta\n",
        "      else:\n",
        "            return 1\n",
        "\n",
        "# Define in the problem statement \n",
        "theta = 0 \n",
        "\n",
        "def bipolar_step(y):\n",
        "      if y < theta:\n",
        "            return theta\n",
        "      else:\n",
        "            return -1\n",
        "\n",
        "def sigmoid_binary(x):\n",
        "      return 1/(1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_bipolar(x):\n",
        "      return (np.exp(x)-1)/(np.exp(x)+1)\n",
        "\n",
        "# Activation function = f(Y) \n",
        "print('--------------------------------------------------------------------------------------------') \n",
        "print('------------------------Activitation Function-----------------------------------------------')\n",
        "print('--------------------------------------------------------------------------------------------')\n",
        "binary_step = binary_step(Y)\n",
        "bipolar_step = bipolar_step(Y)\n",
        "print('The output of a Y is 0.53 and after coverging with binary step activition function result is:' , binary_step) \n",
        "print('The output of a Y is 0.53 and after coverging with bipolar step activition function result is:' , bipolar_step) \n",
        "print('The output of a Y is 0.53 and after coverging with sigmoid binary activition function result is:' , sigmoid_binary(Y)) \n",
        "print('The output of a Y is 0.53 and after coverging with sigmoid bipolar activition function result is:' , sigmoid_bipolar(Y)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "we8K3JgJ8k2j",
        "outputId": "8ce00495-4e71-40da-98cd-6fd9dce37c55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------------------\n",
            "------------------------Finding Y output ---------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "The input of a simple neuron Y for the given network is: [0.8 0.6 0.4]\n",
            "The weight of a simple neuron Y for the given network is: [ 0.1  0.3 -0.2]\n",
            "The output of a simple neuron Y for the given network is: 0.53\n",
            "--------------------------------------------------------------------------------------------\n",
            "------------------------Activitation Function-----------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "The output of a Y is 0.53 and after coverging with binary step activition function result is: 1\n",
            "The output of a Y is 0.53 and after coverging with bipolar step activition function result is: -1\n",
            "The output of a Y is 0.53 and after coverging with sigmoid binary activition function result is: 0.6294831119673949\n",
            "The output of a Y is 0.53 and after coverging with sigmoid bipolar activition function result is: 0.25896622393478974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Program 2: Implementation of single output and Multi output  Perceptron Network with activation function\n",
        "\n",
        "![lab1a.jpg](data:image/jpeg;base64,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)"
      ],
      "metadata": {
        "id": "p_7hiM8H93n1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for the first input pattern x1 = 1, x2 = 1 and t = 1 with weight and bias w1 = 0, w2 = 0 and b = 0\n",
        "import numpy as np\n",
        "#1. Calculate the net input \n",
        "def netinput(x1, x2, weight1, weight2, bias):\n",
        "      Yin = bias + x1*weight1 + x2*weight2\n",
        "      return Yin\n",
        "\n",
        "#2. The output y is computed by applying activiations over the net input calculated\n",
        "def activitionfun(Yin):\n",
        "      if Yin > 0:\n",
        "            return 1 \n",
        "      elif Yin == 0:\n",
        "            return 0\n",
        "      else: \n",
        "            return -1 \n",
        "\n",
        "#3. Check whether target is to f_Yin (calculated output of y)\n",
        "def validation(x1,x2,weight1,weight2,bias, f_Yin, target,aplha):\n",
        "      \n",
        "      if target != f_Yin:\n",
        "      # Change in the weights are: (alpha - learning rate is 1)\n",
        "            weight1 = weight1 + aplha*target*x1\n",
        "            weight2 = weight2 + aplha*target*x2\n",
        "            bias = bias + aplha*target \n",
        "      return weight1,weight2,bias\n",
        "\n",
        "row1 = np.array([[1],[1],[0],[0],[0]])\n",
        "\n",
        "# Calculate the net input for row 1\n",
        "Yin = netinput(row1[0][0],row1[1][0],row1[2][0],row1[3][0],row1[4][0])\n",
        "row1 = np.append(row1,Yin)\n",
        "\n",
        "# Calcuate the output y using activation function\n",
        "Y = activitionfun(Yin)\n",
        "row1 = np.append(row1,Y)\n",
        "\n",
        "#adding target\n",
        "target = 1\n",
        "row1 = np.append(row1,target)\n",
        "\n",
        "# Check any changes in weights and alpha given is 1 as learning rates\n",
        "changes = validation(row1[0],row1[1],row1[2],row1[3],row1[4],row1[6],row1[7],1)\n",
        "row1 = np.append(row1,changes)\n",
        "\n",
        "import pandas as pd \n",
        "df = pd.DataFrame(row1).T\n",
        "\n",
        "df.rename (columns = {0:'x1',1:'x2',2:'weight1',3:'weight2',4:'bias',5:'Yin (Net Input)'}, inplace=True)\n",
        "df.rename (columns = {6:'Calculate Ouput (y)',7:'Target'}, inplace=True)\n",
        "df.rename (columns = {8:'change_weight1',9:'change_weight2',10:'change_bias'}, inplace=True)\n",
        "df\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "X29Xd_lz-AIU",
        "outputId": "ac65c5c1-083c-4cfb-bbc3-765302b9fbda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   x1  x2  weight1  weight2  bias  Yin (Net Input)  Calculate Ouput (y)  \\\n",
              "0   1   1        0        0     0                0                    0   \n",
              "\n",
              "   Target  change_weight1  change_weight2  change_bias  \n",
              "0       1               1               1            1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5bcff3e2-305a-42fc-8bea-475ed9abe86b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>weight1</th>\n",
              "      <th>weight2</th>\n",
              "      <th>bias</th>\n",
              "      <th>Yin (Net Input)</th>\n",
              "      <th>Calculate Ouput (y)</th>\n",
              "      <th>Target</th>\n",
              "      <th>change_weight1</th>\n",
              "      <th>change_weight2</th>\n",
              "      <th>change_bias</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5bcff3e2-305a-42fc-8bea-475ed9abe86b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5bcff3e2-305a-42fc-8bea-475ed9abe86b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5bcff3e2-305a-42fc-8bea-475ed9abe86b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Trying to get full data set again\n",
        "\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import math as math\n",
        "class ActivationFunctions:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    @classmethod\n",
        "    def binary(self, y, theta):\n",
        "\n",
        "        if y >= theta:\n",
        "            return 1\n",
        "        return 0\n",
        "\n",
        "    def bipolar_step(self, y, theta):\n",
        "\n",
        "        if y >= theta:\n",
        "            return 1\n",
        "        return -1\n",
        "\n",
        "    def binary_sigmoidal(self, y):\n",
        "\n",
        "        return 1/(1+math.exp(-y))\n",
        "\n",
        "    def bipolar_sigmoid(self, y):\n",
        "        return math.tanh(y)\n",
        "\n",
        "    def signum(self, y):\n",
        "        if y<0:\n",
        "            return -1\n",
        "        elif y>0:\n",
        "            return 1\n",
        "        return 0\n",
        "\n",
        "def AND_Perceptron(inputs,wt,tgt,epoch,learning_rate,bias,plot=False):\n",
        "    rows = []\n",
        "    X_cols = []\n",
        "    w_cols = []\n",
        "    ocols = ['bias','Y (Net input)','Calculated Output Y']\n",
        "    for xi in range(0,inputs.shape[1]):\n",
        "        X_cols.append(f\"X{xi}\")\n",
        "        w_cols.append(f\"wt{xi}\")\n",
        "    X_cols.append('Target')\n",
        "    X_cols.extend(w_cols)\n",
        "    X_cols.extend(ocols)\n",
        "    cols=X_cols\n",
        "\n",
        "    for epoch in range(0,epoch):\n",
        "        for i in range(0,inputs.shape[0]):\n",
        "            data = []\n",
        "\n",
        "            yin = np.dot(inputs[i],wt) + bias\n",
        "            yout = ActivationFunctions().signum(yin)\n",
        "\n",
        "            data.extend(inputs[i])\n",
        "            data.append(tgt[i])\n",
        "            data.extend(wt)\n",
        "            data.extend([bias,yin,yout])\n",
        "            for j in range(0,inputs.shape[1]):\n",
        "                #print(yout!=tgt[i])\n",
        "                if yout != tgt[i]:\n",
        "                    wt[j] = wt[j] + learning_rate*tgt[i]*inputs[i][j]\n",
        "                    #print(f\"W{j}\", wt[j])\n",
        "                    if j+1 == inputs.shape[1]:\n",
        "                        bias = bias + tgt[i]\n",
        "                        #print(\"B\",bias)\n",
        "            rows.append(data)\n",
        "    #print(rows)\n",
        "    df = pd.DataFrame(rows, columns=cols)\n",
        "    \n",
        "    return df\n",
        "\n",
        "inputs = np.array([[1,1],[1,-1],[-1,1],[-1,-1]])\n",
        "w = np.array([0,0])\n",
        "t = np.array([1,-1,-1,-1])\n",
        "df = AND_Perceptron(inputs=inputs,wt=w,tgt=t,epoch=2,learning_rate=1,bias=0,plot=True)\n",
        "\n",
        "\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "Y2BoQxn7-IJ7",
        "outputId": "fe67ca99-ce44-4226-c403-1ef64c130e8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   X0  X1  Target  wt0  wt1  bias  Y (Net input)  Calculated Output Y\n",
              "0   1   1       1    0    0     0              0                    0\n",
              "1   1  -1      -1    1    1     1              1                    1\n",
              "2  -1   1      -1    0    2     0              2                    1\n",
              "3  -1  -1      -1    1    1    -1             -3                   -1\n",
              "4   1   1       1    1    1    -1              1                    1\n",
              "5   1  -1      -1    1    1    -1             -1                   -1\n",
              "6  -1   1      -1    1    1    -1             -1                   -1\n",
              "7  -1  -1      -1    1    1    -1             -3                   -1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bb42b039-4b6b-4e9b-8cd6-3c31c8790bc0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X0</th>\n",
              "      <th>X1</th>\n",
              "      <th>Target</th>\n",
              "      <th>wt0</th>\n",
              "      <th>wt1</th>\n",
              "      <th>bias</th>\n",
              "      <th>Y (Net input)</th>\n",
              "      <th>Calculated Output Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-3</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-3</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bb42b039-4b6b-4e9b-8cd6-3c31c8790bc0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bb42b039-4b6b-4e9b-8cd6-3c31c8790bc0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bb42b039-4b6b-4e9b-8cd6-3c31c8790bc0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab2: ADALINE Network"
      ],
      "metadata": {
        "id": "GUwBgyZq_gAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Program 1: Implement the classification of OR problem using ADALINE network\n",
        "\n",
        "Implement the classification of OR problem using ADALINE network\n",
        "\n",
        "\n",
        "![Screenshot 2022-12-02 233023.jpg](data:image/jpeg;base64,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)"
      ],
      "metadata": {
        "id": "1OGUhVW1_jyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screenshot 2022-12-02 233433.jpg](data:image/jpeg;base64,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)"
      ],
      "metadata": {
        "id": "p87qXrWCAzhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9-r2PPWtA2a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The initial assumptions for weights and bias are:-\n",
        "1. w1=0.1\n",
        "2. w2=0.1\n",
        "3. alpha=0.1"
      ],
      "metadata": {
        "id": "iz3Fz8gKBDmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1= np.array([1,1,-1,-1])\n",
        "x2= np.array([1,-1,1,-1])\n",
        "t= [1,1,1,-1]\n",
        "\n",
        "\n",
        "error_term_square=[]\n",
        "error_term_final=[]\n",
        "yin=[]\n",
        "w1=0.1\n",
        "w2=0.1\n",
        "bias= 0.1\n",
        "weights_w1=[]\n",
        "weights_w2=[]\n",
        "final_bias=[]\n",
        "  \n",
        "for i in range(1,6):   \n",
        "    for i,j,l in zip(x1,x2,t):\n",
        "    \n",
        "        yin_cal= bias+ ((i*w1)+(j*w2))\n",
        "        yin.append(yin_cal)\n",
        "        \n",
        "        \n",
        "        error_term= (l-yin_cal)\n",
        "        error_term_final.append(error_term)\n",
        "        \n",
        "        \n",
        "        w1=w1+(0.1*(error_term)*i)\n",
        "        weights_w1.append(w1)\n",
        "        \n",
        "        \n",
        "        w2=w2+(0.1*(error_term)*j)\n",
        "        weights_w2.append(w2)\n",
        "        bias=bias+(0.1*error_term)\n",
        "        final_bias.append(bias)\n",
        "        \n",
        "        error_term_square.append(error_term**2)\n",
        "\n",
        "mse=[sum(error_term_square[0:4]),sum(error_term_square[4:8]),sum(error_term_square[8:12]),sum(error_term_square[12:16]),sum(error_term_square[16:])]\n",
        "mse\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8Hj4woCBFYe",
        "outputId": "c7b93d9c-abff-4bb3-f083-09686ffce7cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.02108749,\n",
              " 1.9384466295901872,\n",
              " 1.5493032616885436,\n",
              " 1.4175108182267264,\n",
              " 1.3782253674072138]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1= pd.DataFrame({'Epochs':[1,2,3,4,5],'Total_Mean_Square': mse})\n",
        "df1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "nWGKVq8LBcBz",
        "outputId": "bbfda3ed-35e2-44b5-9b14-51e0dabeeb92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Epochs  Total_Mean_Square\n",
              "0       1           3.021087\n",
              "1       2           1.938447\n",
              "2       3           1.549303\n",
              "3       4           1.417511\n",
              "4       5           1.378225"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3aea8354-73f0-483f-af15-37b4b86329df\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Epochs</th>\n",
              "      <th>Total_Mean_Square</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>3.021087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1.938447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1.549303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1.417511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1.378225</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3aea8354-73f0-483f-af15-37b4b86329df')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3aea8354-73f0-483f-af15-37b4b86329df button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3aea8354-73f0-483f-af15-37b4b86329df');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b=[]\n",
        "for i in range(5):\n",
        "    for j in x1:\n",
        "        b.append(j)\n",
        "c=[]\n",
        "for i in range(5):\n",
        "    for j in x2:\n",
        "        c.append(j)\n",
        "\n",
        "df2= pd.DataFrame({'X1': b,'X2':c})\n",
        "df3=pd.DataFrame({'W1':weights_w1,'W2':weights_w2,'Bias': final_bias,'Error_term':error_term_final,'Error_Squared':error_term_square})\n",
        "\n",
        "z= pd.concat([df2,df3,df1],axis=1)\n",
        "\n",
        "z= z.replace(np.nan,'-')\n",
        "z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "9_JsCCRBBf5Z",
        "outputId": "b2c5d2e3-f97d-4321-d872-b3aa1e7c46fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    X1  X2        W1        W2      Bias  Error_term  Error_Squared Epochs  \\\n",
              "0    1   1  0.170000  0.170000  0.170000    0.700000       0.490000    1.0   \n",
              "1    1  -1  0.253000  0.087000  0.253000    0.830000       0.688900    2.0   \n",
              "2   -1   1  0.161700  0.178300  0.344300    0.913000       0.833569    3.0   \n",
              "3   -1  -1  0.262130  0.278730  0.243870   -1.004300       1.008618    4.0   \n",
              "4    1   1  0.283657  0.300257  0.265397    0.215270       0.046341    5.0   \n",
              "5    1  -1  0.358777  0.225137  0.340517    0.751203       0.564306      -   \n",
              "6   -1   1  0.279465  0.304449  0.419830    0.793123       0.629045      -   \n",
              "7   -1  -1  0.363057  0.388041  0.336238   -0.835916       0.698755      -   \n",
              "8    1   1  0.354323  0.379307  0.327505   -0.087335       0.007627      -   \n",
              "9    1  -1  0.424071  0.309559  0.397252    0.697480       0.486478      -   \n",
              "10  -1   1  0.352345  0.381285  0.468978    0.717259       0.514461      -   \n",
              "11  -1  -1  0.425880  0.454820  0.395444   -0.735348       0.540737      -   \n",
              "12   1   1  0.398266  0.427206  0.367829   -0.276143       0.076255      -   \n",
              "13   1  -1  0.464377  0.361094  0.433940    0.661111       0.437067      -   \n",
              "14  -1   1  0.397442  0.428029  0.500875    0.669342       0.448018      -   \n",
              "15  -1  -1  0.464983  0.495569  0.433334   -0.675403       0.456170      -   \n",
              "16   1   1  0.425594  0.456180  0.393946   -0.393886       0.155146      -   \n",
              "17   1  -1  0.489258  0.392516  0.457610    0.636641       0.405311      -   \n",
              "18  -1   1  0.425345  0.456430  0.521523    0.639132       0.408490      -   \n",
              "19  -1  -1  0.489320  0.520404  0.457548   -0.639748       0.409278      -   \n",
              "\n",
              "   Total_Mean_Square  \n",
              "0           3.021087  \n",
              "1           1.938447  \n",
              "2           1.549303  \n",
              "3           1.417511  \n",
              "4           1.378225  \n",
              "5                  -  \n",
              "6                  -  \n",
              "7                  -  \n",
              "8                  -  \n",
              "9                  -  \n",
              "10                 -  \n",
              "11                 -  \n",
              "12                 -  \n",
              "13                 -  \n",
              "14                 -  \n",
              "15                 -  \n",
              "16                 -  \n",
              "17                 -  \n",
              "18                 -  \n",
              "19                 -  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-00781aaf-f219-4493-ae6e-171dfa2da2d1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>W1</th>\n",
              "      <th>W2</th>\n",
              "      <th>Bias</th>\n",
              "      <th>Error_term</th>\n",
              "      <th>Error_Squared</th>\n",
              "      <th>Epochs</th>\n",
              "      <th>Total_Mean_Square</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.490000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.021087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.253000</td>\n",
              "      <td>0.087000</td>\n",
              "      <td>0.253000</td>\n",
              "      <td>0.830000</td>\n",
              "      <td>0.688900</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.938447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.161700</td>\n",
              "      <td>0.178300</td>\n",
              "      <td>0.344300</td>\n",
              "      <td>0.913000</td>\n",
              "      <td>0.833569</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.549303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.262130</td>\n",
              "      <td>0.278730</td>\n",
              "      <td>0.243870</td>\n",
              "      <td>-1.004300</td>\n",
              "      <td>1.008618</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.417511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.283657</td>\n",
              "      <td>0.300257</td>\n",
              "      <td>0.265397</td>\n",
              "      <td>0.215270</td>\n",
              "      <td>0.046341</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.378225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.358777</td>\n",
              "      <td>0.225137</td>\n",
              "      <td>0.340517</td>\n",
              "      <td>0.751203</td>\n",
              "      <td>0.564306</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.279465</td>\n",
              "      <td>0.304449</td>\n",
              "      <td>0.419830</td>\n",
              "      <td>0.793123</td>\n",
              "      <td>0.629045</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.363057</td>\n",
              "      <td>0.388041</td>\n",
              "      <td>0.336238</td>\n",
              "      <td>-0.835916</td>\n",
              "      <td>0.698755</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.354323</td>\n",
              "      <td>0.379307</td>\n",
              "      <td>0.327505</td>\n",
              "      <td>-0.087335</td>\n",
              "      <td>0.007627</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.424071</td>\n",
              "      <td>0.309559</td>\n",
              "      <td>0.397252</td>\n",
              "      <td>0.697480</td>\n",
              "      <td>0.486478</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.352345</td>\n",
              "      <td>0.381285</td>\n",
              "      <td>0.468978</td>\n",
              "      <td>0.717259</td>\n",
              "      <td>0.514461</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.425880</td>\n",
              "      <td>0.454820</td>\n",
              "      <td>0.395444</td>\n",
              "      <td>-0.735348</td>\n",
              "      <td>0.540737</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.398266</td>\n",
              "      <td>0.427206</td>\n",
              "      <td>0.367829</td>\n",
              "      <td>-0.276143</td>\n",
              "      <td>0.076255</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.464377</td>\n",
              "      <td>0.361094</td>\n",
              "      <td>0.433940</td>\n",
              "      <td>0.661111</td>\n",
              "      <td>0.437067</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.397442</td>\n",
              "      <td>0.428029</td>\n",
              "      <td>0.500875</td>\n",
              "      <td>0.669342</td>\n",
              "      <td>0.448018</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.464983</td>\n",
              "      <td>0.495569</td>\n",
              "      <td>0.433334</td>\n",
              "      <td>-0.675403</td>\n",
              "      <td>0.456170</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.425594</td>\n",
              "      <td>0.456180</td>\n",
              "      <td>0.393946</td>\n",
              "      <td>-0.393886</td>\n",
              "      <td>0.155146</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.489258</td>\n",
              "      <td>0.392516</td>\n",
              "      <td>0.457610</td>\n",
              "      <td>0.636641</td>\n",
              "      <td>0.405311</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.425345</td>\n",
              "      <td>0.456430</td>\n",
              "      <td>0.521523</td>\n",
              "      <td>0.639132</td>\n",
              "      <td>0.408490</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.489320</td>\n",
              "      <td>0.520404</td>\n",
              "      <td>0.457548</td>\n",
              "      <td>-0.639748</td>\n",
              "      <td>0.409278</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-00781aaf-f219-4493-ae6e-171dfa2da2d1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-00781aaf-f219-4493-ae6e-171dfa2da2d1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-00781aaf-f219-4493-ae6e-171dfa2da2d1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Program #2: Implement the classification of XOR problem using MADALINE network\n",
        "\n",
        "\n",
        "Implement the classification of XOR problem using MADALINE network, \n",
        "\n",
        "Initial assumptions for the weights and bias are:-\n",
        "w11=0.05\n",
        "w12=0.1\n",
        "w21=0.2\n",
        "w22=0.2\n",
        "b1=0.3\n",
        "b2=0.15\n",
        "b3=0.5\n",
        "v1=0.5\n",
        "v2=0.5\n",
        "\n",
        "![Screenshot 2022-12-02 234010.jpg](data:image/jpeg;base64,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)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8swl0ZSWBri9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screenshot 2022-12-02 234037.jpg](data:image/jpeg;base64,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)"
      ],
      "metadata": {
        "id": "wIdSyR_uCMEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1=np.array([1,1,-1,-1])\n",
        "x2=np.array([1,-1,1,-1])\n",
        "t=[-1,1,1,-1]\n",
        "\n",
        "w11=0.05\n",
        "w12=0.1\n",
        "w21=0.2\n",
        "w22=0.2\n",
        "b1=0.3\n",
        "b2=0.15\n",
        "b3=0.5\n",
        "v1=0.5\n",
        "v2=0.5\n",
        "final_y= []\n",
        "final_w11=[]\n",
        "final_w12=[]\n",
        "final_w21=[]\n",
        "final_w22=[]\n",
        "final_b1=[]\n",
        "final_b2=[]\n",
        "final_zin_1=[]\n",
        "final_zin_2=[]\n",
        "\n",
        "def activation_function(x):\n",
        "    if(x>=0):\n",
        "        return 1\n",
        "    else:\n",
        "        return -1\n",
        "    \n",
        "for i in range(3):\n",
        "    for i,j,l in zip(x1,x2,t):\n",
        "        # Net input calculation (z1 & z2)\n",
        "        zin_1= b1+((i*w11)+(j*w21))\n",
        "        final_zin_1.append(zin_1)\n",
        "        zin_2= b2+((i*w12)+(j*w22))\n",
        "        final_zin_2.append(zin_2)\n",
        "    \n",
        "        # Activation function\n",
        "    \n",
        "        z1= activation_function(zin_1)\n",
        "        z2=activation_function(zin_2)\n",
        "    \n",
        "        # Net input for output node\n",
        "        y_in= b3+((z1*v1)+(z2*v2))\n",
        "    \n",
        "        # Activation function for final output\n",
        "        y= activation_function(y_in)\n",
        "        final_y.append(y)\n",
        "    \n",
        "        if(y!=l and l==1):\n",
        "            if(abs(zin_2) <abs(zin_1) or min(abs(zin_1),abs(zin_2))==abs(zin_2)):\n",
        "                w22= w22+(0.5*(l-zin_2)*j)\n",
        "                final_w22.append(w22)\n",
        "                w12 = w12+(0.5*(l-zin_2)*i)\n",
        "                final_w12.append(w12)\n",
        "                b2= b2+(0.5*(l-zin_2))\n",
        "                final_b2.append(b2)\n",
        "                w11=w11\n",
        "                final_w11.append(w11)\n",
        "            \n",
        "                w21=w21\n",
        "                final_w21.append(w21)\n",
        "            \n",
        "                b1=b1\n",
        "                final_b1.append(b1)\n",
        "            \n",
        "            else:\n",
        "                w11= w11+(0.5*(l-zin_1)*i)\n",
        "                final_w11.append(w11)\n",
        "                w21=w21+(0.5*(l-zin_1)*j)\n",
        "                final_w21.append(w21)\n",
        "                b1=b1+(0.5*(l-zin_1)*i)\n",
        "                final_b1.append(b1)\n",
        "            \n",
        "                w22=w22\n",
        "                final_w22.append(w22)\n",
        "                w12=w12\n",
        "                final_w12.append(w12)\n",
        "                b2=b2\n",
        "                final_b2.append(b2)\n",
        "           \n",
        "            \n",
        "        \n",
        "        elif(y!=l and l==-1):\n",
        "            if(zin_1>0 and zin_2>0):\n",
        "                w11= w11+(0.5*(l-zin_1)*i)\n",
        "                final_w11.append(w11)\n",
        "                \n",
        "               \n",
        "                w21= w21+(0.5*(l-zin_1)*j)\n",
        "                final_w21.append(w21)\n",
        "                b1= b1+(0.5*(l-zin_1))\n",
        "                final_b1.append(b1)\n",
        "            \n",
        "                w12=w12+(0.5*(l-zin_2)*i)\n",
        "                final_w12.append(w12)\n",
        "                \n",
        "                w22=w22+(0.5*(l-zin_2)*j)\n",
        "                final_w22.append(w22)\n",
        "                b2= b2+(0.5*(l-zin_2))\n",
        "                final_b2.append(b2)\n",
        "                \n",
        "            elif(zin_2>0):\n",
        "                w12=w12+(0.5*(l-zin_2)*i)\n",
        "                final_w12.append(w12)\n",
        "                w22=w22+(0.5*(l-zin_2)*j)\n",
        "                final_w22.append(w22)\n",
        "                b2= b2+(0.5*(l-zin_2))\n",
        "                final_b2.append(b2)\n",
        "                w11=w11\n",
        "                final_w11.append(w11)\n",
        "                \n",
        "                w21=w21\n",
        "                final_w21.append(w21)\n",
        "                b1=b1\n",
        "                final_b1.append(b1)\n",
        "          \n",
        "            elif(zin_1>0):\n",
        "                w11= w11+(0.5*(l-zin_1)*i)\n",
        "                final_w11.append(w11)\n",
        "                w21= w21+(0.5*(l-zin_1)*j)\n",
        "                final_w21.append(w21)\n",
        "                b1= b1+(0.5*(l-zin_1))\n",
        "                final_b1.append(b1)\n",
        "                w22=w22\n",
        "                final_w22.append(w22)\n",
        "                w12=w12\n",
        "                final_w12.append(w12)\n",
        "                b2=b2\n",
        "                final_b2.append(b2)\n",
        "            \n",
        "                \n",
        "            \n",
        "        elif(l==y):\n",
        "                w11=w11\n",
        "                final_w11.append(w11)\n",
        "                w21=w21\n",
        "                final_w21.append(w21)\n",
        "                w12=w12\n",
        "                final_w12.append(w12)\n",
        "                w22=w22\n",
        "                final_w22.append(w22)\n",
        "                b1=b1\n",
        "                final_b1.append(b1)\n",
        "                b2=b2\n",
        "                final_b2.append(b2)\n",
        "       \n",
        "s=[]\n",
        "for i in range(3):\n",
        "    for j in x1:\n",
        "        s.append(j)\n",
        "r=[]\n",
        "for i in range(3):\n",
        "    for j in x2:\n",
        "        r.append(j)\n",
        "w=[]\n",
        "for i in range(3):\n",
        "    for j in t:\n",
        "        w.append(j)\n",
        "\n",
        "df_new_1= pd.DataFrame({'X1': s,'X2': r,'T':w})\n",
        "df_new_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "zaTMbCckCPLn",
        "outputId": "13fe587c-e717-4539-f175-27767b833f47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    X1  X2  T\n",
              "0    1   1 -1\n",
              "1    1  -1  1\n",
              "2   -1   1  1\n",
              "3   -1  -1 -1\n",
              "4    1   1 -1\n",
              "5    1  -1  1\n",
              "6   -1   1  1\n",
              "7   -1  -1 -1\n",
              "8    1   1 -1\n",
              "9    1  -1  1\n",
              "10  -1   1  1\n",
              "11  -1  -1 -1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c2c086be-df20-4598-aa62-89362d88b92e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>T</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c2c086be-df20-4598-aa62-89362d88b92e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c2c086be-df20-4598-aa62-89362d88b92e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c2c086be-df20-4598-aa62-89362d88b92e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_new= pd.DataFrame({'Y':final_y,'Zin_1':final_zin_1,'Zin_2': final_zin_2,'W11':final_w11,'W21': final_w21,'b1': final_b1,'W12':final_w12,'W22': final_w22,'b2':final_b2})\n",
        "df_final_madaline= pd.concat([df_new_1,df_new],axis=1)\n",
        "df_final_madaline"
      ],
      "metadata": {
        "id": "RvfIHfowCeAB",
        "outputId": "5dcc0d72-be30-460a-c18e-6614f788821e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    X1  X2  T  Y     Zin_1     Zin_2       W11       W21        b1       W12  \\\n",
              "0    1   1 -1  1  0.550000  0.450000 -0.725000 -0.575000 -0.475000 -0.625000   \n",
              "1    1  -1  1 -1 -0.625000 -0.675000  0.087500 -1.387500  0.337500 -0.625000   \n",
              "2   -1   1  1 -1 -1.137500 -0.475000  0.087500 -1.387500  0.337500 -1.362500   \n",
              "3   -1  -1 -1  1  1.637500  1.312500  1.406250 -0.068750 -0.981250 -0.206250   \n",
              "4    1   1 -1  1  0.356250  0.168750  0.728125 -0.746875 -1.659375 -0.790625   \n",
              "5    1  -1  1 -1 -0.184375 -3.153125  1.320312 -1.339062 -1.067187 -0.790625   \n",
              "6   -1   1  1 -1 -3.726562 -0.003125  1.320312 -1.339062 -1.067187 -1.292187   \n",
              "7   -1  -1 -1 -1 -1.048438 -1.070313  1.320312 -1.339062 -1.067187 -1.292187   \n",
              "8    1   1 -1 -1 -1.085937 -1.082812  1.320312 -1.339062 -1.067187 -1.292187   \n",
              "9    1  -1  1  1  1.592187 -3.654688  1.320312 -1.339062 -1.067187 -1.292187   \n",
              "10  -1   1  1  1 -3.726562  1.501562  1.320312 -1.339062 -1.067187 -1.292187   \n",
              "11  -1  -1 -1 -1 -1.048438 -1.070313  1.320312 -1.339062 -1.067187 -1.292187   \n",
              "\n",
              "         W22        b2  \n",
              "0  -0.525000 -0.575000  \n",
              "1  -0.525000 -0.575000  \n",
              "2   0.212500  0.162500  \n",
              "3   1.368750 -0.993750  \n",
              "4   0.784375 -1.578125  \n",
              "5   0.784375 -1.578125  \n",
              "6   1.285938 -1.076563  \n",
              "7   1.285938 -1.076563  \n",
              "8   1.285938 -1.076563  \n",
              "9   1.285938 -1.076563  \n",
              "10  1.285938 -1.076563  \n",
              "11  1.285938 -1.076563  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-38323861-97ec-4ad3-8be0-89e14084f241\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>T</th>\n",
              "      <th>Y</th>\n",
              "      <th>Zin_1</th>\n",
              "      <th>Zin_2</th>\n",
              "      <th>W11</th>\n",
              "      <th>W21</th>\n",
              "      <th>b1</th>\n",
              "      <th>W12</th>\n",
              "      <th>W22</th>\n",
              "      <th>b2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>-0.725000</td>\n",
              "      <td>-0.575000</td>\n",
              "      <td>-0.475000</td>\n",
              "      <td>-0.625000</td>\n",
              "      <td>-0.525000</td>\n",
              "      <td>-0.575000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.625000</td>\n",
              "      <td>-0.675000</td>\n",
              "      <td>0.087500</td>\n",
              "      <td>-1.387500</td>\n",
              "      <td>0.337500</td>\n",
              "      <td>-0.625000</td>\n",
              "      <td>-0.525000</td>\n",
              "      <td>-0.575000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1.137500</td>\n",
              "      <td>-0.475000</td>\n",
              "      <td>0.087500</td>\n",
              "      <td>-1.387500</td>\n",
              "      <td>0.337500</td>\n",
              "      <td>-1.362500</td>\n",
              "      <td>0.212500</td>\n",
              "      <td>0.162500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.637500</td>\n",
              "      <td>1.312500</td>\n",
              "      <td>1.406250</td>\n",
              "      <td>-0.068750</td>\n",
              "      <td>-0.981250</td>\n",
              "      <td>-0.206250</td>\n",
              "      <td>1.368750</td>\n",
              "      <td>-0.993750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.356250</td>\n",
              "      <td>0.168750</td>\n",
              "      <td>0.728125</td>\n",
              "      <td>-0.746875</td>\n",
              "      <td>-1.659375</td>\n",
              "      <td>-0.790625</td>\n",
              "      <td>0.784375</td>\n",
              "      <td>-1.578125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.184375</td>\n",
              "      <td>-3.153125</td>\n",
              "      <td>1.320312</td>\n",
              "      <td>-1.339062</td>\n",
              "      <td>-1.067187</td>\n",
              "      <td>-0.790625</td>\n",
              "      <td>0.784375</td>\n",
              "      <td>-1.578125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-3.726562</td>\n",
              "      <td>-0.003125</td>\n",
              "      <td>1.320312</td>\n",
              "      <td>-1.339062</td>\n",
              "      <td>-1.067187</td>\n",
              "      <td>-1.292187</td>\n",
              "      <td>1.285938</td>\n",
              "      <td>-1.076563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1.048438</td>\n",
              "      <td>-1.070313</td>\n",
              "      <td>1.320312</td>\n",
              "      <td>-1.339062</td>\n",
              "      <td>-1.067187</td>\n",
              "      <td>-1.292187</td>\n",
              "      <td>1.285938</td>\n",
              "      <td>-1.076563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1.085937</td>\n",
              "      <td>-1.082812</td>\n",
              "      <td>1.320312</td>\n",
              "      <td>-1.339062</td>\n",
              "      <td>-1.067187</td>\n",
              "      <td>-1.292187</td>\n",
              "      <td>1.285938</td>\n",
              "      <td>-1.076563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.592187</td>\n",
              "      <td>-3.654688</td>\n",
              "      <td>1.320312</td>\n",
              "      <td>-1.339062</td>\n",
              "      <td>-1.067187</td>\n",
              "      <td>-1.292187</td>\n",
              "      <td>1.285938</td>\n",
              "      <td>-1.076563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-3.726562</td>\n",
              "      <td>1.501562</td>\n",
              "      <td>1.320312</td>\n",
              "      <td>-1.339062</td>\n",
              "      <td>-1.067187</td>\n",
              "      <td>-1.292187</td>\n",
              "      <td>1.285938</td>\n",
              "      <td>-1.076563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1.048438</td>\n",
              "      <td>-1.070313</td>\n",
              "      <td>1.320312</td>\n",
              "      <td>-1.339062</td>\n",
              "      <td>-1.067187</td>\n",
              "      <td>-1.292187</td>\n",
              "      <td>1.285938</td>\n",
              "      <td>-1.076563</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-38323861-97ec-4ad3-8be0-89e14084f241')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-38323861-97ec-4ad3-8be0-89e14084f241 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-38323861-97ec-4ad3-8be0-89e14084f241');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab3: Implementation of MLP Network (BPN) with one hidden layer"
      ],
      "metadata": {
        "id": "xW0y8KT9ot5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Program 1: Implement XOR problem using Multilayer Perceptron Network (MLPN)"
      ],
      "metadata": {
        "id": "Csoi9Na3pRII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "3jUXs4OZpeTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "2hM8_u7Fp4rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x): # Returns values that sums to one.\n",
        " return 1 / (1 + np.exp(-x))\n",
        "#derivative of sigmoid\n",
        "def der_sig(x):\n",
        " return x*(1 - x)\n",
        "def accuracy(y_pred, y_true):\n",
        " acc = y_pred.argmax(axis=1) == y_true.argmax(axis=1)\n",
        " return acc.mean()\n",
        "def cost(y, t):\n",
        " return ((t - y)**2).sum() / (2*y.size)\n"
      ],
      "metadata": {
        "id": "cNDhmtmRqEsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[1, 1],\n",
        " [1, 0],\n",
        " [0, 1],\n",
        " [0, 0]])\n",
        "t = np.array([[0], \n",
        " [1],\n",
        " [1],\n",
        " [0]])\n",
        "lr = 0.1\n",
        "hidden_size = 2\n",
        "output_size = 1\n",
        "results = pd.DataFrame(columns=[\"mse\", \"accuracy\"])"
      ],
      "metadata": {
        "id": "K2fBO3owqGeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_data, input_size = X.shape"
      ],
      "metadata": {
        "id": "896aaHIWqNyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing weight for the hidden layer\n",
        "W1 = np.random.random(size=(input_size, hidden_size)) \n",
        "# initializing weight for the output layer\n",
        "W2 = np.random.random(size=(hidden_size , output_size)) "
      ],
      "metadata": {
        "id": "BeS3CgqJqRiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"W1 = \", W1, \"\\n\\n\\nW2 = \", W2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xB9aFoAJqU8u",
        "outputId": "bb000273-a9a5-471d-d579-36b814cdd0e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1 =  [[0.73811652 0.71526564]\n",
            " [0.2425563  0.80816107]] \n",
            "\n",
            "\n",
            "W2 =  [[0.73474302]\n",
            " [0.2430053 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 10000\n",
        "for n in range(epoch):\n",
        " \n",
        " #print(\"\\nInput \", n+1)\n",
        " \n",
        " \n",
        " #feed foward\n",
        " #input to hidden\n",
        " hidden_in = np.dot(X, W1) \n",
        " hidden_out = sigmoid(hidden_in)\n",
        " #print(\"\\nhidden out = \", hidden_out)\n",
        " \n",
        " #hidden to output\n",
        " output_in = np.dot(hidden_out, W2) \n",
        " output_out = sigmoid(output_in)\n",
        " #print(\"\\noutput out = \", output_out)\n",
        " \n",
        " total_cost = cost(output_out, t)\n",
        " BPN_error = t - output_out\n",
        " #print(\"\\nBPN error = \", BPN_error)\n",
        " \n",
        " acc = accuracy(output_out, t)\n",
        " results=results.append({\"mse\":total_cost, \"accuracy\":acc},ignore_index=True )\n",
        " \n",
        " #BACKPROPAGATION\n",
        " \n",
        " \n",
        " \n",
        " output_error = (-BPN_error) * der_sig(output_out)\n",
        " output_delta = np.dot(output_out.T, output_error ) \n",
        " #print(\"\\nOutput delta = \", output_delta)\n",
        " \n",
        " hidden_error = np.dot(output_error, W2.T)\n",
        " #print(\"\\nHidden error = \", hidden_error)\n",
        " \n",
        " hidden_delta = np.dot(X.T, hidden_error * der_sig(hidden_out))\n",
        " #print(\"\\nHidden delta = \", hidden_delta)\n",
        " \n",
        " \n",
        " # update weights\n",
        " #W2 += lr * hidden_out.T.dot(output_delta)\n",
        " W2 -= lr * output_delta\n",
        " #print(\"\\nW2 = \", W2)\n",
        " #b2 += np.sum(output_delta,axis=0,keepdims=True) * lr\n",
        " \n",
        " #W1 += lr * np.dot(hidden_delta.T, X)\n",
        " W1 -= lr * hidden_delta\n",
        " #print(\"\\nW1 = \", W1)\n",
        " #b1 += lr * hidden_delta\n",
        " #print(\"---------------------------\")\n",
        " \n",
        "print(\"Final output weights = \", W2)\n",
        "print(\"\\nFinal input weights = \", W1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKlTyHk0qbDY",
        "outputId": "1af1e7fb-ffb0-475e-dd57-f79feaed5e5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final output weights =  [[ 0.20961721]\n",
            " [-0.28212051]]\n",
            "\n",
            "Final input weights =  [[2.59752072 0.12942183]\n",
            " [2.55264075 0.33980013]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results.mse.plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "glFnbk5qqmAk",
        "outputId": "52833d01-19bc-41f4-9f9d-944e7727fa31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f2209a3b130>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 4: MLP with multiple hidden layers"
      ],
      "metadata": {
        "id": "f6LAZ_Gy2Y22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example"
      ],
      "metadata": {
        "id": "VPGptzCf1sAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = pd.read_csv('/content/diabetes.csv')\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "DhAEhERdyv2q",
        "outputId": "7115168f-beee-4088-d283-7ed88d45072c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "0              6      148             72             35        0  33.6   \n",
              "1              1       85             66             29        0  26.6   \n",
              "2              8      183             64              0        0  23.3   \n",
              "3              1       89             66             23       94  28.1   \n",
              "4              0      137             40             35      168  43.1   \n",
              "..           ...      ...            ...            ...      ...   ...   \n",
              "763           10      101             76             48      180  32.9   \n",
              "764            2      122             70             27        0  36.8   \n",
              "765            5      121             72             23      112  26.2   \n",
              "766            1      126             60              0        0  30.1   \n",
              "767            1       93             70             31        0  30.4   \n",
              "\n",
              "     DiabetesPedigreeFunction  Age  Outcome  \n",
              "0                       0.627   50        1  \n",
              "1                       0.351   31        0  \n",
              "2                       0.672   32        1  \n",
              "3                       0.167   21        0  \n",
              "4                       2.288   33        1  \n",
              "..                        ...  ...      ...  \n",
              "763                     0.171   63        0  \n",
              "764                     0.340   27        0  \n",
              "765                     0.245   30        0  \n",
              "766                     0.349   47        1  \n",
              "767                     0.315   23        0  \n",
              "\n",
              "[768 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a9aa2b1b-c0e5-4a62-8cd9-1360f62667fa\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>10</td>\n",
              "      <td>101</td>\n",
              "      <td>76</td>\n",
              "      <td>48</td>\n",
              "      <td>180</td>\n",
              "      <td>32.9</td>\n",
              "      <td>0.171</td>\n",
              "      <td>63</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>2</td>\n",
              "      <td>122</td>\n",
              "      <td>70</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "      <td>36.8</td>\n",
              "      <td>0.340</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>5</td>\n",
              "      <td>121</td>\n",
              "      <td>72</td>\n",
              "      <td>23</td>\n",
              "      <td>112</td>\n",
              "      <td>26.2</td>\n",
              "      <td>0.245</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>1</td>\n",
              "      <td>126</td>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.1</td>\n",
              "      <td>0.349</td>\n",
              "      <td>47</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>1</td>\n",
              "      <td>93</td>\n",
              "      <td>70</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>30.4</td>\n",
              "      <td>0.315</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>768 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a9aa2b1b-c0e5-4a62-8cd9-1360f62667fa')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a9aa2b1b-c0e5-4a62-8cd9-1360f62667fa button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a9aa2b1b-c0e5-4a62-8cd9-1360f62667fa');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Build a MLP architecture with two or more hidden layers\n",
        "#Use any open data set from kaggle or public forum\n",
        "#Perform train, validation and test split of data set\n",
        "#Compute the cost for all epochs and plot (either MSE or accuracy) (refer fig)\n",
        "\n",
        "X= data.loc[:, data.columns != 'Outcome']\n",
        "y= data['Outcome']\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "uX5Zy9Z0zFWv",
        "outputId": "59bd7dd0-b916-426c-d46f-c1d1188848a9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "0              6      148             72             35        0  33.6   \n",
              "1              1       85             66             29        0  26.6   \n",
              "2              8      183             64              0        0  23.3   \n",
              "3              1       89             66             23       94  28.1   \n",
              "4              0      137             40             35      168  43.1   \n",
              "..           ...      ...            ...            ...      ...   ...   \n",
              "763           10      101             76             48      180  32.9   \n",
              "764            2      122             70             27        0  36.8   \n",
              "765            5      121             72             23      112  26.2   \n",
              "766            1      126             60              0        0  30.1   \n",
              "767            1       93             70             31        0  30.4   \n",
              "\n",
              "     DiabetesPedigreeFunction  Age  \n",
              "0                       0.627   50  \n",
              "1                       0.351   31  \n",
              "2                       0.672   32  \n",
              "3                       0.167   21  \n",
              "4                       2.288   33  \n",
              "..                        ...  ...  \n",
              "763                     0.171   63  \n",
              "764                     0.340   27  \n",
              "765                     0.245   30  \n",
              "766                     0.349   47  \n",
              "767                     0.315   23  \n",
              "\n",
              "[768 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-26ab85d6-f59b-486a-a84e-edb09f42f3c1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>10</td>\n",
              "      <td>101</td>\n",
              "      <td>76</td>\n",
              "      <td>48</td>\n",
              "      <td>180</td>\n",
              "      <td>32.9</td>\n",
              "      <td>0.171</td>\n",
              "      <td>63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>2</td>\n",
              "      <td>122</td>\n",
              "      <td>70</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "      <td>36.8</td>\n",
              "      <td>0.340</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>5</td>\n",
              "      <td>121</td>\n",
              "      <td>72</td>\n",
              "      <td>23</td>\n",
              "      <td>112</td>\n",
              "      <td>26.2</td>\n",
              "      <td>0.245</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>1</td>\n",
              "      <td>126</td>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.1</td>\n",
              "      <td>0.349</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>1</td>\n",
              "      <td>93</td>\n",
              "      <td>70</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>30.4</td>\n",
              "      <td>0.315</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>768 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-26ab85d6-f59b-486a-a84e-edb09f42f3c1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-26ab85d6-f59b-486a-a84e-edb09f42f3c1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-26ab85d6-f59b-486a-a84e-edb09f42f3c1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) "
      ],
      "metadata": {
        "id": "0izLzKZazSz3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import MLPClassifer \n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Create model object\n",
        "clf = MLPClassifier(hidden_layer_sizes=(6,5),\n",
        "                    random_state=5,\n",
        "                    verbose=True,\n",
        "                    learning_rate_init=0.01,\n",
        "                   max_iter=10)\n",
        "\n",
        "# Fit data onto the model\n",
        "clf.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zztUd6fzXMW",
        "outputId": "c2ebf81a-6823-4b84-b3b0-5c263790a475"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(6, 5), learning_rate_init=0.01, max_iter=10,\n",
              "              random_state=5, verbose=True)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make prediction on test dataset\n",
        "ypred=clf.predict(X_test)\n",
        "\n",
        "# Import accuracy score \n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Calcuate accuracy\n",
        "accuracy_score(y_test,ypred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhSB4FifzeNO",
        "outputId": "bf92901e-9423-439c-8901-4eb920b17b63"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.670995670995671"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hence the accuracy score is 67.09%% for the following parameters:-\n",
        "2 hidden layers(with 6 and 5 neurons respectively) and learning rate =0.01"
      ],
      "metadata": {
        "id": "TqzeAS3NzuF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Plotting epochs vs mse\n",
        "print(y.shape,ypred.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-27W_fKzvtA",
        "outputId": "d9739d3b-5d19-43c1-825e-1e527a387506"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(768,) (231,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "  \n",
        "\n",
        "mean_squared_error(y_test,ypred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mLu-7G2z2xb",
        "outputId": "ed82325a-cd3c-466d-e338-b95edbae3a41"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.329004329004329"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs= [i for i in range(1,200)]\n",
        "mse_test=[]\n",
        "accuracy_test=[]\n",
        "accuracy_train=[]\n",
        "mse_train=[]\n",
        "for i in epochs:\n",
        "    clf = MLPClassifier(hidden_layer_sizes=(6,5),\n",
        "                    random_state=5,\n",
        "                    verbose=True,\n",
        "                    learning_rate_init=0.01,\n",
        "                   max_iter=i)\n",
        "    clf.fit(X_train,y_train)\n",
        "    ypred1=clf.predict(X_test)\n",
        "    ypred2=clf.predict(X_train)\n",
        "    \n",
        "    c= (accuracy_score(y_test,ypred1))\n",
        "    accuracy_test.append(c)\n",
        "    d= mean_squared_error(y_test,ypred1)\n",
        "    mse_test.append(d)\n",
        "    e= (accuracy_score(y_train,ypred2))\n",
        "    accuracy_train.append(e)\n",
        "    f= mean_squared_error(y_train,ypred2)\n",
        "    mse_train.append(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpwIFVRt0AEt",
        "outputId": "b40891fe-e055-4508-8987-bc3d8bbf37b4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (3) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (4) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (6) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (7) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (8) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (9) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (11) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (12) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (13) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (14) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (16) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (17) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (18) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (19) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (21) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (22) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (23) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (24) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (25) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (26) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (27) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (28) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (29) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (31) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (32) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (33) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (34) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (35) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (36) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (37) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (38) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (39) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (40) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (41) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (42) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (43) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (44) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (45) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (46) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (47) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 1, loss = 3.50268808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (48) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (49) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 1, loss = 3.50268808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (52) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (53) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (54) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (55) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (56) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (57) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (58) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (59) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (60) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (61) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (62) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (63) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (64) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (65) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (66) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (67) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (68) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (69) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (70) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (71) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (72) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (73) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (74) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (75) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (76) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (77) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (78) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (79) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (81) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (82) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (83) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (84) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (85) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (86) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (87) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (88) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (89) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (90) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (91) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (92) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (93) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (94) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (95) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (96) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (97) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (98) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (99) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (101) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (102) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (103) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (104) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (105) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (106) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (107) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (108) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (109) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (110) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (111) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (112) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (113) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (114) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (115) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (116) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (117) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (118) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (119) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (120) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (121) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (122) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (123) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (124) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (125) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (126) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (127) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (128) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (129) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (130) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (131) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (132) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (133) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (134) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (135) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (136) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (137) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (138) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (139) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (140) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (141) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (142) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (143) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (144) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (145) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (146) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 1, loss = 3.50268808"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (147) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (148) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (149) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (151) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (152) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (153) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (154) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (155) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (156) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (157) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (158) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (159) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (160) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (161) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (162) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (163) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (164) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (165) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (166) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (167) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (168) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (169) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (170) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.50268808\n",
            "Iteration 2, loss = 2.51544592\n",
            "Iteration 3, loss = 1.88134983\n",
            "Iteration 4, loss = 1.49278940\n",
            "Iteration 5, loss = 1.33966138\n",
            "Iteration 6, loss = 1.11791867\n",
            "Iteration 7, loss = 0.95353581\n",
            "Iteration 8, loss = 0.88832868\n",
            "Iteration 9, loss = 0.82566624\n",
            "Iteration 10, loss = 0.77463976\n",
            "Iteration 11, loss = 0.72384017\n",
            "Iteration 12, loss = 0.65873405\n",
            "Iteration 13, loss = 0.67236608\n",
            "Iteration 14, loss = 0.67852332\n",
            "Iteration 15, loss = 0.66290119\n",
            "Iteration 16, loss = 0.63247044\n",
            "Iteration 17, loss = 0.64852641\n",
            "Iteration 18, loss = 0.63009972\n",
            "Iteration 19, loss = 0.60671224\n",
            "Iteration 20, loss = 0.62711716\n",
            "Iteration 21, loss = 0.61267642\n",
            "Iteration 22, loss = 0.61188207\n",
            "Iteration 23, loss = 0.59994722\n",
            "Iteration 24, loss = 0.60144127\n",
            "Iteration 25, loss = 0.59390700\n",
            "Iteration 26, loss = 0.59356292\n",
            "Iteration 27, loss = 0.59034321\n",
            "Iteration 28, loss = 0.58489237\n",
            "Iteration 29, loss = 0.58622369\n",
            "Iteration 30, loss = 0.58559266\n",
            "Iteration 31, loss = 0.57726480\n",
            "Iteration 32, loss = 0.57912590\n",
            "Iteration 33, loss = 0.58053618\n",
            "Iteration 34, loss = 0.58385777\n",
            "Iteration 35, loss = 0.57399644\n",
            "Iteration 36, loss = 0.57575678\n",
            "Iteration 37, loss = 0.56896217\n",
            "Iteration 38, loss = 0.57560655\n",
            "Iteration 39, loss = 0.57284779\n",
            "Iteration 40, loss = 0.56985514\n",
            "Iteration 41, loss = 0.56922624\n",
            "Iteration 42, loss = 0.56347394\n",
            "Iteration 43, loss = 0.55505760\n",
            "Iteration 44, loss = 0.55756140\n",
            "Iteration 45, loss = 0.55256821\n",
            "Iteration 46, loss = 0.55136219\n",
            "Iteration 47, loss = 0.55451229\n",
            "Iteration 48, loss = 0.54829121\n",
            "Iteration 49, loss = 0.54679155\n",
            "Iteration 50, loss = 0.54725961\n",
            "Iteration 51, loss = 0.56035465\n",
            "Iteration 52, loss = 0.54403215\n",
            "Iteration 53, loss = 0.54156627\n",
            "Iteration 54, loss = 0.54434606\n",
            "Iteration 55, loss = 0.54283043\n",
            "Iteration 56, loss = 0.53554273\n",
            "Iteration 57, loss = 0.53671403\n",
            "Iteration 58, loss = 0.53764173\n",
            "Iteration 59, loss = 0.53399194\n",
            "Iteration 60, loss = 0.53245090\n",
            "Iteration 61, loss = 0.53068436\n",
            "Iteration 62, loss = 0.52898700\n",
            "Iteration 63, loss = 0.52571291\n",
            "Iteration 64, loss = 0.52389572\n",
            "Iteration 65, loss = 0.52417697\n",
            "Iteration 66, loss = 0.52249907\n",
            "Iteration 67, loss = 0.52030987\n",
            "Iteration 68, loss = 0.52022751\n",
            "Iteration 69, loss = 0.52006658\n",
            "Iteration 70, loss = 0.51569859\n",
            "Iteration 71, loss = 0.51779396\n",
            "Iteration 72, loss = 0.51486668\n",
            "Iteration 73, loss = 0.51088483\n",
            "Iteration 74, loss = 0.51227327\n",
            "Iteration 75, loss = 0.50942678\n",
            "Iteration 76, loss = 0.51088763\n",
            "Iteration 77, loss = 0.50603925\n",
            "Iteration 78, loss = 0.50607648\n",
            "Iteration 79, loss = 0.50529983\n",
            "Iteration 80, loss = 0.50354887\n",
            "Iteration 81, loss = 0.50428741\n",
            "Iteration 82, loss = 0.50243426\n",
            "Iteration 83, loss = 0.50009380\n",
            "Iteration 84, loss = 0.49909041\n",
            "Iteration 85, loss = 0.49986642\n",
            "Iteration 86, loss = 0.49930802\n",
            "Iteration 87, loss = 0.49795232\n",
            "Iteration 88, loss = 0.49656583\n",
            "Iteration 89, loss = 0.49513752\n",
            "Iteration 90, loss = 0.49432006\n",
            "Iteration 91, loss = 0.49318764\n",
            "Iteration 92, loss = 0.49388662\n",
            "Iteration 93, loss = 0.49511804\n",
            "Iteration 94, loss = 0.49266226\n",
            "Iteration 95, loss = 0.49101997\n",
            "Iteration 96, loss = 0.48995940\n",
            "Iteration 97, loss = 0.49141925\n",
            "Iteration 98, loss = 0.48784781\n",
            "Iteration 99, loss = 0.48777309\n",
            "Iteration 100, loss = 0.48516804\n",
            "Iteration 101, loss = 0.48705625\n",
            "Iteration 102, loss = 0.48266380\n",
            "Iteration 103, loss = 0.48471847\n",
            "Iteration 104, loss = 0.48308483\n",
            "Iteration 105, loss = 0.48390210\n",
            "Iteration 106, loss = 0.49089954\n",
            "Iteration 107, loss = 0.48550391\n",
            "Iteration 108, loss = 0.48363931\n",
            "Iteration 109, loss = 0.48853219\n",
            "Iteration 110, loss = 0.47980773\n",
            "Iteration 111, loss = 0.47984265\n",
            "Iteration 112, loss = 0.47675979\n",
            "Iteration 113, loss = 0.47491658\n",
            "Iteration 114, loss = 0.47802357\n",
            "Iteration 115, loss = 0.47336540\n",
            "Iteration 116, loss = 0.47718714\n",
            "Iteration 117, loss = 0.47523168\n",
            "Iteration 118, loss = 0.46719881\n",
            "Iteration 119, loss = 0.47720641\n",
            "Iteration 120, loss = 0.47041857\n",
            "Iteration 121, loss = 0.46875256\n",
            "Iteration 122, loss = 0.47456919\n",
            "Iteration 123, loss = 0.47436645\n",
            "Iteration 124, loss = 0.47435025\n",
            "Iteration 125, loss = 0.46587680\n",
            "Iteration 126, loss = 0.47629674\n",
            "Iteration 127, loss = 0.46699504\n",
            "Iteration 128, loss = 0.47874149\n",
            "Iteration 129, loss = 0.47902627\n",
            "Iteration 130, loss = 0.46254953\n",
            "Iteration 131, loss = 0.47659160\n",
            "Iteration 132, loss = 0.46785949\n",
            "Iteration 133, loss = 0.46425374\n",
            "Iteration 134, loss = 0.46592729\n",
            "Iteration 135, loss = 0.46304108\n",
            "Iteration 136, loss = 0.46423590\n",
            "Iteration 137, loss = 0.46530588\n",
            "Iteration 138, loss = 0.46531537\n",
            "Iteration 139, loss = 0.46041215\n",
            "Iteration 140, loss = 0.46322939\n",
            "Iteration 141, loss = 0.45927647\n",
            "Iteration 142, loss = 0.46149706\n",
            "Iteration 143, loss = 0.46047258\n",
            "Iteration 144, loss = 0.45953875\n",
            "Iteration 145, loss = 0.46409272\n",
            "Iteration 146, loss = 0.46612588\n",
            "Iteration 147, loss = 0.46260394\n",
            "Iteration 148, loss = 0.45986397\n",
            "Iteration 149, loss = 0.46155042\n",
            "Iteration 150, loss = 0.46156493\n",
            "Iteration 151, loss = 0.45783514\n",
            "Iteration 152, loss = 0.45696526\n",
            "Iteration 153, loss = 0.45556420\n",
            "Iteration 154, loss = 0.45438755\n",
            "Iteration 155, loss = 0.45493089\n",
            "Iteration 156, loss = 0.45684456\n",
            "Iteration 157, loss = 0.45903152\n",
            "Iteration 158, loss = 0.45523096\n",
            "Iteration 159, loss = 0.45832321\n",
            "Iteration 160, loss = 0.45100345\n",
            "Iteration 161, loss = 0.45378214\n",
            "Iteration 162, loss = 0.45996795\n",
            "Iteration 163, loss = 0.45672003\n",
            "Iteration 164, loss = 0.46169311\n",
            "Iteration 165, loss = 0.45427275\n",
            "Iteration 166, loss = 0.45822164\n",
            "Iteration 167, loss = 0.46697074\n",
            "Iteration 168, loss = 0.46281527\n",
            "Iteration 169, loss = 0.46311878\n",
            "Iteration 170, loss = 0.46128337\n",
            "Iteration 171, loss = 0.46584400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "index_accuracy_test= accuracy_test.index(max(accuracy_test))\n",
        "print(index_accuracy_test)\n",
        "finallist1= [epochs[155],round(accuracy_test[155],3)]\n",
        "print(finallist1)\n",
        "\n",
        "# Train\n",
        "index_accuracy= accuracy_train.index(max(accuracy_train))\n",
        "print(index_accuracy)\n",
        "finallist2= [epochs[155],round(accuracy_test[155],3)]\n",
        "print(finallist2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQTpZjA20O6Z",
        "outputId": "2e43c767-0be9-4b7b-c9ff-4ad6a566bb32"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "155\n",
            "[156, 0.788]\n",
            "169\n",
            "[156, 0.788]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###  Peak of training and testing error touched when no of epochs = 156 and accuracy score= 0.788.\n",
        "plt.plot(epochs,accuracy_test,color='g')\n",
        "plt.plot(epochs,accuracy_train,color='r')\n",
        "plt.title('No of epochs vs Accuracy')\n",
        "plt.ylabel('Accuracy score')\n",
        "plt.xlabel('No of Epochs')\n",
        "plt.legend(labels=['test','train'])\n",
        "plt.annotate(finallist1,xy=finallist1)\n",
        "plt.annotate(finallist2,xy=finallist2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "63xzKFSN0WmY",
        "outputId": "88201644-58f4-4ae8-bdeb-a5ee8f56c323"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(156, 0.788, '[156, 0.788]')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MSE lowest for test\n",
        "index_mse= mse_test.index(min(mse_test))\n",
        "print(index_mse)\n",
        "finallist= [epochs[155],round(mse_test[155],3)]\n",
        "print(finallist)\n",
        "# MSE lowest for train\n",
        "index_mse_train= mse_train.index(min(mse_train))\n",
        "print(index_mse_train)\n",
        "finallist_train= [epochs[169],round(mse_train[169],3)]\n",
        "print(finallist_train)\n",
        "\n",
        "plt.plot(epochs,mse_test,color='orange')\n",
        "plt.plot(epochs,mse_train,color='blue')\n",
        "\n",
        "plt.title('No of epochs vs MSE')\n",
        "plt.ylabel('MSE')\n",
        "plt.xlabel('No of Epochs')\n",
        "\n",
        "plt.annotate('For test lowest mse :'+ str(finallist),xy= finallist,xytext=(50,0.375))\n",
        "plt.annotate('For train lowest mse:'+str(finallist_train),xy=finallist_train,xytext=(50,0.350))\n",
        "plt.legend(labels=['test','train'])\n",
        ";"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "fG3C-TiJ0rlt",
        "outputId": "407c9733-95a8-4174-d0b2-2d1d2f6ddcf3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "155\n",
            "[156, 0.212]\n",
            "169\n",
            "[170, 0.21]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Hence the minimal mse for testing data is attained at 156th epoch and the value for mse : 0.212\n",
        "### A the minimal mse for training data is attained at 170th epoch and the value for mse : 0.2"
      ],
      "metadata": {
        "id": "TPJ55_ek07j8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_names= X.columns\n",
        "col_names\n",
        "\n",
        "# a) Pre-processing\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc= StandardScaler()\n",
        "\n",
        "X= sc.fit_transform(X)\n",
        "\n",
        "X= pd.DataFrame(X,columns= col_names)\n",
        "X\n",
        "\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "model = ExtraTreesClassifier()\n",
        "model.fit(X,y)\n",
        "print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
        "#plot graph of feature importances for better visualization\n",
        "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
        "feat_importances.nlargest(5).plot(kind='bar')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "iL7DZWWD1F8M",
        "outputId": "75a987cd-d2f2-42a6-8bd8-f077b206b3d6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.10930995 0.23043073 0.09720183 0.07996884 0.07549186 0.14319823\n",
            " 0.12047322 0.14392535]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X= X[['Glucose','BMI','Age','DiabetesPedigreeFunction','Pregnancies']]\n",
        "X\n",
        "\n",
        "MLPClassifier()\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "  'activation':['relu','softmax','linear'],\n",
        "    'learning_rate_init':[0.1,0.05,0.03,0.02],\n",
        "    'max_iter':[100,200,50,20,30],\n",
        "    'hidden_layer_sizes':[(6,5),(3,6),(4,2),(8,9),(50,60),(100,200)]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(clf, param_grid = param_grid, cv = 5, verbose = 5, n_jobs = -1)\n",
        "grid.fit(X_train,y_train)\n",
        "\n",
        "best_estimator = grid.best_estimator_\n",
        "\n",
        "best_estimator\n",
        "\n",
        "best_pred_y = best_estimator.predict(X_test)\n",
        "print(\"Accuracy: {}%\".format(accuracy_score(y_test, best_pred_y)*100))\n",
        "\n",
        "### Hence the accuracy score is : 76.19%, which is better than the MLP model before data-preprocessing and hypertuning "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRArXlnv1Qkw",
        "outputId": "3dd60855-7819-4e15-e47d-905fcef17622"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 360 candidates, totalling 1800 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "1200 fits failed out of a total of 1800.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "600 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 752, in fit\n",
            "    return self._fit(X, y, incremental=False)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 427, in _fit\n",
            "    self._fit_stochastic(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 635, in _fit_stochastic\n",
            "    batch_loss, coef_grads, intercept_grads = self._backprop(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 307, in _backprop\n",
            "    inplace_derivative = DERIVATIVES[self.activation]\n",
            "KeyError: 'softmax'\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "600 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 752, in fit\n",
            "    return self._fit(X, y, incremental=False)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 384, in _fit\n",
            "    self._validate_hyperparameters()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 493, in _validate_hyperparameters\n",
            "    raise ValueError(\n",
            "ValueError: The activation 'linear' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.65176532 0.65176532 0.65176532 0.64991346 0.65176532 0.68722741\n",
            " 0.68722741 0.70212876 0.67410869 0.68541018 0.70778816 0.71152648\n",
            " 0.70212876 0.63691589 0.65735549 0.7059017  0.70029422 0.72272413\n",
            " 0.65917272 0.70385947 0.64991346 0.64991346 0.64991346 0.64991346\n",
            " 0.64991346 0.64991346 0.64991346 0.64991346 0.64991346 0.64991346\n",
            " 0.6443233  0.6443233  0.6443233  0.64619245 0.6443233  0.66476289\n",
            " 0.71306681 0.64622707 0.65740741 0.64250606 0.64430599 0.64430599\n",
            " 0.64991346 0.64991346 0.64991346 0.64991346 0.64991346 0.64991346\n",
            " 0.64991346 0.64991346 0.65546902 0.66843198 0.64991346 0.64991346\n",
            " 0.64991346 0.64991346 0.64991346 0.64991346 0.63134303 0.64991346\n",
            " 0.64804431 0.64804431 0.64804431 0.64991346 0.64804431 0.64991346\n",
            " 0.64991346 0.65178262 0.65178262 0.6573901  0.64248875 0.64248875\n",
            " 0.62942195 0.63880235 0.65       0.71886466 0.72634129 0.69094843\n",
            " 0.63502942 0.64619245 0.64991346 0.64991346 0.64991346 0.64991346\n",
            " 0.64991346 0.68158532 0.68158532 0.67597785 0.65917272 0.67227414\n",
            " 0.66853583 0.66853583 0.65920734 0.65552094 0.60884389 0.70195569\n",
            " 0.71491866 0.68338525 0.66673589 0.67040498 0.65365178 0.65365178\n",
            " 0.65365178 0.64991346 0.64991346 0.66486674 0.66486674 0.66860505\n",
            " 0.66112842 0.6573901  0.66857044 0.66857044 0.64240222 0.64430599\n",
            " 0.64614053 0.67589131 0.67589131 0.64974039 0.66844929 0.6647802\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 11.58030216\n",
            "Iteration 2, loss = 3.58741917\n",
            "Iteration 3, loss = 2.30374157\n",
            "Iteration 4, loss = 1.98224947\n",
            "Iteration 5, loss = 1.78091299\n",
            "Iteration 6, loss = 1.28869028\n",
            "Iteration 7, loss = 1.12610291\n",
            "Iteration 8, loss = 1.00073464\n",
            "Iteration 9, loss = 0.80652535\n",
            "Iteration 10, loss = 0.81114227\n",
            "Iteration 11, loss = 0.77150121\n",
            "Iteration 12, loss = 0.74501229\n",
            "Iteration 13, loss = 0.72376546\n",
            "Iteration 14, loss = 0.68231710\n",
            "Iteration 15, loss = 0.66301803\n",
            "Iteration 16, loss = 0.65002186\n",
            "Iteration 17, loss = 0.63718215\n",
            "Iteration 18, loss = 0.63088754\n",
            "Iteration 19, loss = 0.61861063\n",
            "Iteration 20, loss = 0.60301273\n",
            "Iteration 21, loss = 0.59875216\n",
            "Iteration 22, loss = 0.58812763\n",
            "Iteration 23, loss = 0.58139141\n",
            "Iteration 24, loss = 0.57245044\n",
            "Iteration 25, loss = 0.56815673\n",
            "Iteration 26, loss = 0.55235168\n",
            "Iteration 27, loss = 0.55330371\n",
            "Iteration 28, loss = 0.54322088\n",
            "Iteration 29, loss = 0.53705311\n",
            "Iteration 30, loss = 0.53287977\n",
            "Iteration 31, loss = 0.53606653\n",
            "Iteration 32, loss = 0.53555167\n",
            "Iteration 33, loss = 0.53125635\n",
            "Iteration 34, loss = 0.53180406\n",
            "Iteration 35, loss = 0.52538106\n",
            "Iteration 36, loss = 0.52046817\n",
            "Iteration 37, loss = 0.52128056\n",
            "Iteration 38, loss = 0.51054168\n",
            "Iteration 39, loss = 0.51589399\n",
            "Iteration 40, loss = 0.50958834\n",
            "Iteration 41, loss = 0.50490058\n",
            "Iteration 42, loss = 0.50593996\n",
            "Iteration 43, loss = 0.49776983\n",
            "Iteration 44, loss = 0.49751702\n",
            "Iteration 45, loss = 0.49729197\n",
            "Iteration 46, loss = 0.49565407\n",
            "Iteration 47, loss = 0.48824771\n",
            "Iteration 48, loss = 0.48639385\n",
            "Iteration 49, loss = 0.48534982\n",
            "Iteration 50, loss = 0.48485856\n",
            "Iteration 51, loss = 0.50330512\n",
            "Iteration 52, loss = 0.51772832\n",
            "Iteration 53, loss = 0.51577423\n",
            "Iteration 54, loss = 0.49319800\n",
            "Iteration 55, loss = 0.48148618\n",
            "Iteration 56, loss = 0.48445465\n",
            "Iteration 57, loss = 0.48180840\n",
            "Iteration 58, loss = 0.47570639\n",
            "Iteration 59, loss = 0.47632515\n",
            "Iteration 60, loss = 0.47598890\n",
            "Iteration 61, loss = 0.47393233\n",
            "Iteration 62, loss = 0.47968099\n",
            "Iteration 63, loss = 0.48222618\n",
            "Iteration 64, loss = 0.48834624\n",
            "Iteration 65, loss = 0.48027520\n",
            "Iteration 66, loss = 0.46654713\n",
            "Iteration 67, loss = 0.48062513\n",
            "Iteration 68, loss = 0.47925545\n",
            "Iteration 69, loss = 0.47690657\n",
            "Iteration 70, loss = 0.47644117\n",
            "Iteration 71, loss = 0.47408704\n",
            "Iteration 72, loss = 0.47524824\n",
            "Iteration 73, loss = 0.46206782\n",
            "Iteration 74, loss = 0.46277102\n",
            "Iteration 75, loss = 0.46763930\n",
            "Iteration 76, loss = 0.46119256\n",
            "Iteration 77, loss = 0.47269472\n",
            "Iteration 78, loss = 0.48568140\n",
            "Iteration 79, loss = 0.46716965\n",
            "Iteration 80, loss = 0.47137308\n",
            "Iteration 81, loss = 0.46028356\n",
            "Iteration 82, loss = 0.46015997\n",
            "Iteration 83, loss = 0.45947855\n",
            "Iteration 84, loss = 0.46725080\n",
            "Iteration 85, loss = 0.46041423\n",
            "Iteration 86, loss = 0.45964593\n",
            "Iteration 87, loss = 0.45150721\n",
            "Iteration 88, loss = 0.45458717\n",
            "Iteration 89, loss = 0.45281423\n",
            "Iteration 90, loss = 0.48234532\n",
            "Iteration 91, loss = 0.49470540\n",
            "Iteration 92, loss = 0.45196104\n",
            "Iteration 93, loss = 0.45039622\n",
            "Iteration 94, loss = 0.45550822\n",
            "Iteration 95, loss = 0.49586570\n",
            "Iteration 96, loss = 0.45376647\n",
            "Iteration 97, loss = 0.48715994\n",
            "Iteration 98, loss = 0.49264910\n",
            "Iteration 99, loss = 0.48965860\n",
            "Iteration 100, loss = 0.49408601\n",
            "Iteration 101, loss = 0.46003356\n",
            "Iteration 102, loss = 0.47065588\n",
            "Iteration 103, loss = 0.45919117\n",
            "Iteration 104, loss = 0.45648753\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(8, 9), learning_rate_init=0.02,\n",
              "              random_state=5, verbose=True)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example"
      ],
      "metadata": {
        "id": "qVo6A4sTe0w_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural network has been implemented from basics without use of any framework like TensorFlow or sci-kit-learn.\n",
        "# Training has been done on the MNIST dataset. Implementation has been done with minimum use of libraries to get a \n",
        "# better understanding of the concept and working on neural nets.\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "im_JmR182hwk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The Neural Net will work better when the X values are between 0 and 1 in this case, \n",
        "# and we need to turn the Y values (integers between 0 and 9) into categorical, \n",
        "# because at the end we want the probabilities of every image being each number.\n",
        "\n",
        "xx , yy = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "X = xx\n",
        "Y = yy\n",
        "X = np.array(X.values/255) #Bring X values between 0 and 1 by dividing X by 255\n",
        "Y = np.array(Y.values) #The target, it is a vector of numbers between 0 and 9\n",
        "m = X.shape[0]  #number of images\n",
        "n = X.shape[1] + 1  #number of pixels in each image, +1 because of the bias \n",
        "X = np.concatenate((np.ones([m,1]),X), axis=1) # Adding the column of 1's for supporting the bias\n",
        "Y = Y.astype(np.int64)\n",
        "cat = np.zeros([m,10])\n",
        "\n",
        "# We need to turn Y into a categorical matrix:\n",
        "\n",
        "for ind, num in enumerate(Y):\n",
        "    cat[ind][num] = 1\n",
        "Y = cat # Y is now cathegorical-> ex: 3 = [0,0,0,1,0,0,0,0,0,0]\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.15, random_state=42) "
      ],
      "metadata": {
        "id": "5L3KzskW7oYd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following methodology was adopted:\n",
        "\n",
        "Functions for random initialization of weights and bias\n",
        "Activation functions\n",
        "Derivatives of the activation function\n",
        "Function for Forwarding propagation\n",
        "Backward propagation\n",
        "The cost has been written separately and derivative has been found."
      ],
      "metadata": {
        "id": "4YZrtLSr81do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('SHAPES:')\n",
        "print(f'- X original: {X.shape[0]} x {X.shape[1]}')\n",
        "print(f'- Y original: {Y.shape[0]} x {Y.shape[1]}')\n",
        "print(f'- X Train: {x_train.shape[0]} x {x_train.shape[1]}')\n",
        "print(f'- Y train: {y_train.shape[0]} x {y_train.shape[1]}')\n",
        "print(f'- X Test: {x_test.shape[0]} x {x_test.shape[1]}')\n",
        "print(f'- Y Test: {y_test.shape[0]} x {y_test.shape[1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loa-G95R7y-1",
        "outputId": "9c8c02ee-863e-4610-e45b-b18795bb4ffd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SHAPES:\n",
            "- X original: 70000 x 785\n",
            "- Y original: 70000 x 10\n",
            "- X Train: 59500 x 785\n",
            "- Y train: 59500 x 10\n",
            "- X Test: 10500 x 785\n",
            "- Y Test: 10500 x 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NumeroAleatorio = np.random.randint(0,59500)\n",
        "plt.imshow(np.reshape(x_train[NumeroAleatorio][1:], [28,28]), cmap='Greys')\n",
        "print(np.argmax(y_train[NumeroAleatorio]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "6qqtV7Ni73AV",
        "outputId": "d0942c51-7ed1-487b-ee0f-5ff722c79af0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTalgorithm:\n",
        "    def __init__(self, estrutura, epochs=10, alpha=0.1, batch_size=0, Regularizer=0.0):\n",
        "        self.tamanhos = [x for x in estrutura if isinstance(x, int)]\n",
        "        self.ativações = [x.lower() for x in estrutura if isinstance(x, str)]\n",
        "        self.inputs = [epochs,alpha,batch_size,Regularizer]\n",
        "        self.epochs = epochs\n",
        "        self.alpha = alpha\n",
        "        self.Reg = Regularizer\n",
        "        self.batch_size = batch_size\n",
        "        self.Thetas = self.Initilization()\n",
        "        self.CheckInputs()\n",
        "    \n",
        "    def Initilization(self):\n",
        "        Thetas = {}\n",
        "        for layer in range(len(self.tamanhos)-1):\n",
        "            Thetas[f'T{layer+1}'] = np.random.randn(self.tamanhos[layer]+1, self.tamanhos[layer+1])/10\n",
        "        return Thetas\n",
        "\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1/(1 + np.exp(-z))\n",
        "\n",
        "    def dxsigmoid(self, z):\n",
        "        return np.multiply(self.sigmoid(z),(1-self.sigmoid(z)))\n",
        "\n",
        "    def softmax(self,x):\n",
        "        x = np.array(x)\n",
        "        e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return (e_x / e_x.sum(axis=1)[:,None])\n",
        "\n",
        "    def Relu(self, x):\n",
        "        return np.maximum(0,x)\n",
        "    \n",
        "    def dxRelu(self, x):\n",
        "        return np.where(x >= 0, 1, 0)\n",
        "    \n",
        "\n",
        "    def Forward(self,X):\n",
        "\n",
        "        X = np.matrix(X)\n",
        "        m = X.shape[0]\n",
        "        att = self.ativações\n",
        "        Thetas = self.Thetas\n",
        "        Forward_steps = {}\n",
        "        Forward_steps['a1'] = X\n",
        "        Ultimo_layer = int(len(self.tamanhos))\n",
        "        for layer in range(1,Ultimo_layer):\n",
        "            Forward_steps[f'z{layer+1}'] = np.dot(Forward_steps[f'a{layer}'], Thetas[f'T{layer}'])\n",
        "            if att[layer-1] == 'sigmoid':\n",
        "                Forward_steps[f'a{layer+1}'] = np.concatenate((np.ones([m,1]), self.sigmoid(Forward_steps[f'z{layer+1}'])), axis=1)\n",
        "            elif att[layer-1] == 'softmax':\n",
        "                Forward_steps[f'a{layer+1}'] = np.concatenate((np.ones([m,1]), self.softmax(Forward_steps[f'z{layer+1}'])), axis=1)\n",
        "            elif att[layer-1] == 'relu':\n",
        "                Forward_steps[f'a{layer+1}'] = np.concatenate((np.ones([m,1]), self.Relu(Forward_steps[f'z{layer+1}'])), axis=1)\n",
        "            else:\n",
        "                print(\"Error\")\n",
        "        \n",
        "        h = Forward_steps.pop(f'a{Ultimo_layer}')\n",
        "        Forward_steps['h'] = h[:,1:]\n",
        "        \n",
        "        return Forward_steps\n",
        "        \n",
        "\n",
        "    def CostFunction(self):\n",
        "        Y = self.Y\n",
        "        X = self.X\n",
        "        Thetas = self.Thetas\n",
        "        m = self.m\n",
        "        Reg = self.Reg\n",
        "        soma_weights = 0\n",
        "        for i in range(len(Thetas)):\n",
        "            weights = Thetas[f'T{i+1}']\n",
        "            weights[0] = 0\n",
        "            soma_weights += np.sum(weights**2)\n",
        "        Forward_dict = self.Forward(X)\n",
        "        h = Forward_dict['h']\n",
        "        soma = np.sum((np.multiply(-Y , np.log(h)) - np.multiply((1-Y),(np.log(1-h)))))\n",
        "        J = soma/m + (Reg/(2*m)) * soma_weights\n",
        "        return J\n",
        "\n",
        "\n",
        "    def Gradients(self,X,Y):\n",
        "        X = np.matrix(X)\n",
        "        Y = np.matrix(Y)\n",
        "        m = X.shape[0]\n",
        "        Thetas = self.Thetas\n",
        "        n_layers = len(self.tamanhos)\n",
        "        att = self.ativações\n",
        "        Thetas_grad = []\n",
        "\n",
        "        Forward_list = self.Forward(X)\n",
        "        deltas = {}\n",
        "        deltas[f'delta{n_layers}'] = Forward_list['h'] - Y # delta4\n",
        "        for i in range(n_layers-1,1,-1):# 3 ... 2\n",
        "            if att[i-2] == 'sigmoid':\n",
        "                deltas[f'delta{i}'] = np.multiply((np.dot(deltas[f'delta{i+1}'],Thetas[f'T{i}'][1:].T)) , self.dxsigmoid(Forward_list[f'z{i}']))\n",
        "            elif att[i-2] == 'relu':\n",
        "                deltas[f'delta{i}'] = np.multiply((np.dot(deltas[f'delta{i+1}'],Thetas[f'T{i}'][1:].T)) , self.dxRelu(Forward_list[f'z{i}']))\n",
        "        \n",
        "        for c in range(len(deltas)):#0 ... 1 ... 2\n",
        "            BigDelta = np.array(np.dot(deltas[f'delta{c+2}'].T, Forward_list[f'a{c+1}']))\n",
        "            weights = Thetas[f'T{c+1}']\n",
        "            weights[0] = 0\n",
        "            grad = np.array(BigDelta + (self.Reg * weights.T))/m\n",
        "            Thetas_grad.append(grad)\n",
        "        return Thetas_grad #[T1_grad, T2_grad, T3_grad]\n",
        "\n",
        "\n",
        "    def Precisao(self, X,Y):\n",
        "\n",
        "        Forward_list = self.Forward(X)\n",
        "        h = Forward_list['h']\n",
        "        y_hat = np.argmax(h, axis=1)[:,None]\n",
        "        y = np.argmax(Y, axis=1)[:,None]\n",
        "        return np.mean(y_hat == y)\n",
        "\n",
        "\n",
        "    def train(self, X, Y,x_test,y_test):\n",
        "\n",
        "        Thetas = self.Thetas\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.m = X.shape[0]\n",
        "        j_history = []\n",
        "        sec1 = time.time()\n",
        "        if self.batch_size <= 0:\n",
        "            b_size = self.m\n",
        "            print(f'batch per shape {b_size}..')\n",
        "        elif isinstance(self.batch_size, int) and (1<= self.batch_size <= self.m):\n",
        "            b_size = self.batch_size\n",
        "        else:\n",
        "            return \n",
        "        for ep in range(self.epochs):\n",
        "            m = self.m\n",
        "            a = np.array([0,b_size])\n",
        "            num = 1\n",
        "\n",
        "            for i in range(m // b_size):\n",
        "                inx = a + b_size*i\n",
        "                grad_list = self.Gradients(X[inx[0]:inx[1]], Y[inx[0]:inx[1]])\n",
        "                for g in range(len(grad_list)):\n",
        "                    self.Thetas[f'T{g+1}'] = self.Thetas[f'T{g+1}'] - self.alpha*np.array(grad_list[g]).T\n",
        "            \n",
        "            if (ep+1) % num == 0: #\n",
        "                J = self.CostFunction()\n",
        "                j_history.append(J)\n",
        "                accu_train = self.Precisao(X,Y)\n",
        "                accu_test = self.Precisao(x_test,y_test)\n",
        "                sec2 = time.time()\n",
        "                tempo_gasto = sec2 - sec1\n",
        "                print(f'Epoch: {ep+1}; Cost: {J:.5f}: Accuracy Train: {accu_train:.5%}; Accuracy Test: {accu_test:.5%}; Tempo Gasto: {tempo_gasto:.2f} s')\n",
        "        return j_history, self.Thetas"
      ],
      "metadata": {
        "id": "A8XLX6D879pV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MNISTalgorithm([784, 'relu', 200,'sigmoid',80, 'softmax', 10], epochs=10, alpha=0.1, batch_size=100)"
      ],
      "metadata": {
        "id": "FhynCYzk7-5t"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "j_history, trained_thetas = model.train(x_train,y_train,x_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdaeYEJc8B0I",
        "outputId": "c34d910f-8e3e-4e84-bc85-dd870daad758"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1; Cost: 0.64065: Accuracy Train: 89.71092%; Accuracy Test: 89.61905%; Tempo Gasto: 9.07 s\n",
            "Epoch: 2; Cost: 0.47077: Accuracy Train: 92.05714%; Accuracy Test: 92.03810%; Tempo Gasto: 17.12 s\n",
            "Epoch: 3; Cost: 0.39149: Accuracy Train: 93.42689%; Accuracy Test: 93.13333%; Tempo Gasto: 27.79 s\n",
            "Epoch: 4; Cost: 0.33755: Accuracy Train: 94.35462%; Accuracy Test: 93.97143%; Tempo Gasto: 42.28 s\n",
            "Epoch: 5; Cost: 0.29692: Accuracy Train: 95.07227%; Accuracy Test: 94.43810%; Tempo Gasto: 55.03 s\n",
            "Epoch: 6; Cost: 0.26534: Accuracy Train: 95.63866%; Accuracy Test: 94.89524%; Tempo Gasto: 64.47 s\n",
            "Epoch: 7; Cost: 0.23980: Accuracy Train: 96.04706%; Accuracy Test: 95.20000%; Tempo Gasto: 77.90 s\n",
            "Epoch: 8; Cost: 0.21843: Accuracy Train: 96.43361%; Accuracy Test: 95.37143%; Tempo Gasto: 91.91 s\n",
            "Epoch: 9; Cost: 0.20030: Accuracy Train: 96.77143%; Accuracy Test: 95.65714%; Tempo Gasto: 100.61 s\n",
            "Epoch: 10; Cost: 0.18462: Accuracy Train: 97.05882%; Accuracy Test: 95.99048%; Tempo Gasto: 110.56 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(j_history, 'go-',label='Cost')\n",
        "plt.title('Cost per Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Cost')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "jdrn85cX8JVh",
        "outputId": "7b6c056a-6666-496b-add3-bdbe0c9ec311"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 5: Implementation of Convolution Neural Network"
      ],
      "metadata": {
        "id": "oS7bDpopuzyB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3DFp4ADFu-rv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}